---
layout: post
title: Recommender system for contraceptive products
subtitle: based on side-effects
tags: [webmd, scrape websites, crawl websites, requests, html, beautifulsoup, python, sentiment analysis, review, contraception, birth control]
---

## Background
Virtually all sexually-experienced women in the US, about [53 million](https://www.cdc.gov/nchs/data/nhsr/nhsr062.pdf) in 2006-2010, have used contraception at some point in their lives. The range of options for female contraception have steadily improved over the years though the pills remain the most popular method.

The hormonal methods, while highly-effective and reversible, have also been known to have a range of side effects from [headaches, weight gain, mood swings, depression](https://www.nexplanon.com/side-effects/) to [venous thrombosis](http://labeling.pfizer.com/ShowLabeling.aspx?id=666). Discontinuation of the offending method and switching are common. In fact, side effects were the most frequently cited reason for [discontinuation](https://www.cdc.gov/nchs/data/nhsr/nhsr062.pdf) of the contraceptive injection, contraceptive patch and the pills. It is a trial and error process where women have to try out different methods, different brands until they find one that “works for them”. 

## Goals
Develop a recommender system to help women choose hormonal contraceptive products based on other women’s experience.
For example: Julie has good experience with Liletta (Levonorgestrel), good experience with Gianvi but bad experience with Nikki (even though they’re both Drospirenone/Ethinyl estradiol). Hailey has good experience with Liletta and bad experience with Nikki so Gianvi might work for her.
This is a very much simplified scenario and certainly does not replace physicians’ opinions.

## Approach and Data source
There is no existing open data source for my purpose. There are websites with reviews of contraception products (webmd, drugs.com) but the reviews are not linked to user ids. Then there are forums like Reddit where users talk about their experiences like Reddit but there are no formal review data. So my approach is to combine these two types of sources:

1. First, train an NLP classifier to identify positive experience versus negative experience using reviews from webmd, drugs.com etc.
2. Then scrape for discussions about hormonal contraceptive products on Reddit and a few other forums like patient.info, acne.org etc. 
3. Build a database of users and their experience with different contraceptive methods
4. Build recommender system using newly created database

## Deliverables
- Recommender system for contraceptive methods with a website interface
Additional features that will be nice to have:
- FDA API link for contraceptive products’ information
- Incorporate contraindication into the recommender system so certain products would not be recommended
- Forms for users to enter their own experience to improve the accuracy of the recommender system.

## Progress
I've finished the first step: crawling through 3021 webmd.com pages and obtained 14,452 reviews on contraveptive products. I've also trained an NLP classifier using SVM to identify positive experience versus negative experience. Using TI-IDF, these are the top 20 words for positive versus negative reviews.

![pos](/img/positive.png)
![neg](/img/negative.png)
![num](/img/num.png)

The problem I'm facing is that there are a lot more positive reviews than negative reviews and there are a lot of reviews for Mirena compared to other products, which skewed the result. I'm getting 77% accuracy on cross-validation and 76% on my test set. I'm hoping to get more data from other websites like drug.com and research more into how to address the skewed data problem.

## The code
```python
import os
import matplotlib.pyplot as plt
import requests
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import re
import math
import string
import collections

import nltk
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.snowball import EnglishStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import PCA
from scipy import sparse as sp
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import cross_val_score


url ='https://www.webmd.com/drugs/2/condition-3454/pregnancy%20contraception'
base_url = 'https://www.webmd.com'
r = requests.get(url)
html_doc = r.text
soup = BeautifulSoup(html_doc, "html5lib")

# extract links to review pages
links = [link.get('href') for link in soup.find_all('a', href=re.compile("drugreview"))]
# add base_url
links = [base_url + link for link in links]
# only keep links that have a review
links = links[0:269]
# 269 products

def num_pages(product_links):
    num_list = []
    remove_list = []
    for link in product_links:
        r = requests.get(link)
        html_doc = r.text
        soup = BeautifulSoup(html_doc, "html5lib")
        try:
            # get one string because duplicates
            num_page = [post.get_text() for post in soup.find_all(class_='postPaging')][0]
            start = 'of'
            end = 'Next'
            # number of reviews
            num = num_page[(num_page.find(start)+len(start)):num_page.rfind(end)]
            # number of review pages, given each page has 5 reviews
            num = math.ceil(int(num)/5)
        except:
            remove_link = product_links.index(link)
            remove_list.append(int(remove_link))
            continue
        num_list.append(int(num))

    return num_list, remove_list

# get list of number of pages for each product
# product_page_num, r_list = num_pages(links)
# product_page_num = [299, 118, 80, 90, 68, 110, 82, 92, 77, 54, 39, 72, 49, 42, 37, 45, 51, 37, 37, 37, 34, 36, 30, 30, 27, 37, 24, 34, 27, 23, 24, 29, 29, 30, 22, 25, 19, 25, 14, 21, 24, 26, 23, 14, 20, 24, 19, 19, 20, 14, 19, 20, 14, 11, 12, 5, 14, 16, 17, 13, 9, 13, 11, 11, 14, 10, 10, 13, 9, 9, 12, 12, 10, 9, 7, 7, 8, 7, 10, 7, 7, 5, 7, 7, 8, 10, 7, 6, 10, 7, 7, 7, 7, 6, 6, 5, 7, 5, 4, 5, 5, 5, 2, 5, 5, 5, 4, 6, 4, 5, 5, 2, 4, 3, 3, 5, 3, 3, 4, 5, 3, 3, 4, 3, 4, 2, 2, 3, 2, 4, 2, 2, 2, 3, 3, 3, 2, 3, 1, 3, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]

sum(product_page_num)
# r_list = [198, 243]
# remove the products with no review page
# links = np.delete(links, r_list).tolist()
# 267



def extract_reviews(review_link):
    """
    given a review link, this function will go to the page and extract comment, rating
    :param review_link: string
    :return: comments and ratings
    """
    # navigate to the review page to scrape reviews
    r = requests.get(review_link)
    html_doc = r.text
    soup = BeautifulSoup(html_doc, "html5lib")

    # extract full reviews
    full_reviews = [post.get_text() for post in soup.find_all('p', class_='comment',
                                                                id=['comFull1', 'comFull2', 'comFull3', 'comFull4',
                                                                    'comFull5'])]
    # remove words in the front and the back
    full_reviews = [post[8:-17] for post in full_reviews]

    # extract ratings
    ratings = [post.get_text() for post in soup.find_all('p', class_='inlineRating starRating')]
    # get ratings for 'Satisfaction' only: exclude the first 3 items as they're dummies, then take the 3rd reviews
    ratings = ratings[3::3]
    # remove words in the front and only get the ratings
    ratings = [rating[-1] for rating in ratings]

    return full_reviews, ratings

def get_review_link(link_product, num_page):
    """
    Function to get review links for each product. 299 is the highest number of review pages (Mirena)
    :param link_product: link of each product
    :param num_page: number of review pages for each product
    :return: list of links to all review pages of the product
    """
    link_product_list = []
    # eg. there are 299 pages for Mirena
    for i in range(num_page):
        link = link_product + '&pageIndex=' + str(i)
        link_product_list.append(link)
    return link_product_list

def combine_reviews(product_list, product_page_n):
    """
    crawl through all product links to get all reviews and combine into list of reviews and list of ratings
    :param product_list: list of contraceptive product links to be scraped
    :return review_df: dataframe of reviews and ratings
    """
    reviews = []
    ratings = []
    for i, link in enumerate(product_list):
        # list of review pages for each product
        link_list = get_review_link(link, product_page_n[i])
        for link_review in link_list:
            review, rating = extract_reviews(link_review)
            reviews.append(review)
            ratings.append(rating)
    flat_list = [item for sublist in reviews for item in sublist]
    flat_list_ratings = [item for sublist in ratings for item in sublist]
    review_df = pd.DataFrame({'ratings':flat_list_ratings, 'reviews': flat_list })
    return review_df

# scrape all reviews for all products
df = combine_reviews(links, product_page_num)
# export to csv file
df.to_csv('reviews.csv', index=False )

### machine learning step ###
## preprocessing
# function to replace punctuations, digits, and white spaces with blanks
def process_text(variable):
    """
    do text processing: lower case, remove digits, punctuations, white space characters, stop words
    :param variable: Pandas series in a dataframe
    :return: Pandas series in a dataframe
    """
    # remove stop words and words with 3 characters or less
    stop_words = list(stopwords.words('english'))
    more_words = ['birth','control','month','period','pill','start','take','time','week','year']
    # 179 words
    remove_words = stop_words + more_words

    # stemming words
    sno = EnglishStemmer()

    # use str method on series to get lowercase
    variable = variable.str.lower()

    # use maketrans to create translation table to replace punctuations, digits and white spaace characters (eg. \r) with blanks
    # maketrans map each character to another character in a string so it doesnt work on words. That's also why we have to have the same number of characters
    remove_char = str.maketrans(string.punctuation + string.digits + string.whitespace,
                                ' ' * len(string.punctuation + string.digits + string.whitespace))
    # maketrans table requires strings with similar lengths
    variable = variable.str.translate(remove_char)

    # create list of list for text
    text_list = [x.split() for x in variable]

    # remove and stem words using nested list comprehension
    text_list_clean = [[sno.stem(w) for w in line if (w not in remove_words) and (len(w) >= 4)] for line in text_list]

    # add clean text back to dataframe
    text_series = pd.Series(text_list_clean)
    text_series = text_series.apply(lambda x: ' '.join(x))

    return text_series

# df['reviews_clean'] = process_text(df['reviews'])

# recode: 1 is positive, 0 is negative
df['ratings_new'] = df.ratings.apply(lambda x: 1 if np.logical_or(x=='5', x=='4') else 0)
df['ratings_new'].value_counts().plot('bar', title = 'Number of positive versus negative reviews')
plt.xticks([0,1], ('positive','negative'), rotation='horizontal')

## split into train and test sets
x_train, x_test, y_train, y_test = train_test_split(df['reviews'], df['ratings_new']
                                                    , test_size=0.2, random_state=203)
# add stemming for sklearn


# train model using SVM
# make all words lower case, tokenize alphanumerical words that have 2 characters or more
text_clf_svm = Pipeline([('vect', CountVectorizer(lowercase=True, stop_words='english', ngram_range=(1, 3),
                                                  min_df=0.0025, max_df=.1, max_features=None, strip_accents='unicode',
                                                  analyzer="word", token_pattern=r'(?u)\b\w\w+\b'
                                                  # , tokenizer = SnowballTokenizer()
                                                  )),
                         ('tfidf', TfidfTransformer(smooth_idf=1)),
                         ('clf-svm', SGDClassifier(loss='modified_huber',
                                                   # penalty='l2',
                                                   # regularization term
                                                   # class_weight='balanced',
                                                   alpha=1e-3, tol=1e-3, max_iter=50))])
text_clf_svm = text_clf_svm.fit(x_train, y_train)

# cross validation results
cv_results_svm = cross_val_score(text_clf_svm, x_train, y_train, cv=5)
print("Cross-validation accuracy: %0.2f (+/- %0.2f)" % (cv_results_svm.mean(), cv_results_svm.std() * 2))

# test result
accuracy_svm = text_clf_svm.score(x_test, y_test)
print('Test accuracy ', accuracy_svm)




##########################################################
sno = EnglishStemmer()
positive = df[df.ratings_new == 1]
negative = df[df.ratings_new == 0]
positive['stemmed'] = positive.reviews.map(lambda x: ' '.join([sno.stem(y) for y in x.split(' ')]))
positive.stemmed.head()

negative['stemmed'] = negative.reviews.map(lambda x: ' '.join([sno.stem(y) for y in x.split(' ')]))
negative.stemmed.head()

cvec_pos = CountVectorizer(stop_words='english', min_df=0.0025, max_df=.1, ngram_range=(1,2))
cvec_neg = CountVectorizer(stop_words='english', min_df=0.0025, max_df=.1, ngram_range=(1,2))
# Calculate all the n-grams found in all documents
from itertools import islice
cvec_pos.fit(positive.stemmed)
cvec_neg.fit(negative.stemmed)
list(islice(cvec_pos.vocabulary_.items(), 20))
# Check how many total n-grams we have
len(cvec_pos.vocabulary_)

# Let’s look at the top 20 most common terms
cvec_counts_pos = cvec_pos.transform(positive.stemmed)
cvec_counts_neg = cvec_neg.transform(negative.stemmed)
# occ = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()
# counts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ})
# counts_df.sort_values(by='occurrences', ascending=False).head(20)

# use tifidf
transformer = TfidfTransformer()
transformed_pos = transformer.fit_transform(cvec_counts_pos)
transformed_neg = transformer.fit_transform(cvec_counts_neg)


# take a look at the top 20 terms by average tf-idf weight:
weights_pos = np.asarray(transformed_pos.mean(axis=0)).ravel().tolist()
weights_df_pos = pd.DataFrame({'term': cvec_pos.get_feature_names(), 'weight': weights_pos})
weights_df_pos.sort_values(by='weight', ascending=False).head(20)

weights_neg = np.asarray(transformed_neg.mean(axis=0)).ravel().tolist()
weights_df_neg = pd.DataFrame({'term': cvec_neg.get_feature_names(), 'weight': weights_neg})
weights_df_neg.sort_values(by='weight', ascending=False).head(20)

weights_df_pos.sort_values(by='weight', ascending=False).head(20).plot('term','weight', kind='bar'
                                                                       , title='Top 20 terms by tf-idf for positive reviews')

weights_df_neg.sort_values(by='weight', ascending=False).head(20).plot('term','weight', kind='bar'
                                                                       , title='Top 20 terms by tf-idf for negative reviews')

```
