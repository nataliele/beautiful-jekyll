---
layout: post
title: A Natural Language Processing method for classifying cancer pathology reports
# subtitle: NAACCR Cancer informatics hackathon 2018
tags: [nlp, svm, cnn, python, cancer, pathology reports]
---

## The state of cancer registry
There are a lot of inefficiencies in healthcare. Case in point: In cancer registry, each state receives up to 300,000 pathology reports per year with several hundreds data elements to capture. These reports are manually classified and abstracted by cancer registrars at the hospital level, state registry level or national cancer registry level. So one of the challenges at the NAACCR hackathon was to build an NLP to classify the anatomical sites of cancer from each pathology report.

This is simple enough, but cancer registry requires 100% accuracy. You can tweak and mold the NLP model as much as you want and can get to say 95%. But how do you know where the 5% incorrect classifications are? In real life there is no test set and there is no way to know which 50 of the 1000 reports you're getting wrong.

My pitch to the judges was to employ a two-pronged approach: We build an NLP method that incorporates two algorithms: Support Vector Machine (SVM) and Convolutional Neural Networks (CNN). Each of these algorithm by itself are about 94% accurate on the validation set. We will then print out the records where the two algorithms disagree. These are the ambiguous reports that need attention from an actual trained cancer registrar. The cancer registrar will then review the reports and decide the final classification for the reports. By using NLP, we can improve efficiency tremendously; yet, we're still able to maintain quality and accuracy by having a cancer registrar doing the review on the toughest cases.

After the presentation, I have so many cancer registrars coming to me expressing their enthusiasm and endorsing the idea of having machine learning and human working in tandem. My team got second prize in the end but the overwhelmingly positive feedbacks were a clear sign of opportunities in cancer registry in particular and healthcare in general.



## The codes
Done using Python

### Preprocessing
```python
import numpy as np
import pandas as pd
import string
import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk import FreqDist


clinical_texts = []
# with open('data/data_table.txt', 'r') as f:
with open('data/data_test.txt', 'r') as f:
    for line in f:
        # clinical_texts.append(line[1:])
        clinical_texts.append(line[0:])

# clinical_texts_lower=clinical_texts.lower()


# remove punctuation from each word
clinical_texts_clean = []
# table = str.maketrans('', '', string.punctuation)
remove_punc = str.maketrans(string.punctuation, ' '*len(string.punctuation))
# remove_digits = str.maketrans('', '', string.digits)
remove_digits = str.maketrans(string.digits, ' '*len(string.digits))
# stop_words = str.maketrans('','',)
for line in clinical_texts:
    line_lower = line.lower()
    stripped = line_lower.translate(remove_punc)
    stripped_digit = stripped.translate(remove_digits)
    words = stripped_digit.split()
    clinical_texts_clean.append(words)

stop_words = set(stopwords.words('english'))
more_words = ['name','carcinoma','diagnosis','date','tumor','clinical','biopsy','clinicalhistory','history']
clean_text = []
for line in clinical_texts_clean:
    word_list = []
    for w in line:
        if (w not in stop_words) and (w not in more_words) and (len(w)>=4) :
            word_list.append(w)
    clean_text.append(word_list)
```

### Word embedding
```python
import os
import sys
import numpy as np

import pandas as pd
from keras.preprocessing import text
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.layers import Dense, Input, GlobalMaxPooling1D, Activation, Flatten
from keras.layers import Conv1D, MaxPooling1D, Embedding,Dropout
from keras.models import Model
from keras.models import Sequential
from keras.preprocessing.text import Tokenizer

import matplotlib.pyplot as plt

# parameters
MAX_SEQUENCE_LENGTH = 1000
MAX_NUM_WORDS = 20000       # maximum number of words to keep, based on word frequency
EMBEDDING_DIM = 300         #dimension of the pre-trained word vectors
VALIDATION_SPLIT = 0.2


print('Indexing word vectors.')

embeddings_index = {}
with open(os.path.join(GLOVE_DIR, 'glove.6B.300d.txt'), encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word  = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
        # embeddings_index is a dictionary with the key being a word and the value being an array of embedding values
        #400,000 word vectors

print('Found %s word vectors.' % len(embeddings_index))
```

### Convolutional Neural Networks
```
clinical_texts = clean_text[1:]
labels = labels[1:]
print('Found %s clinical texts.' % len(clinical_texts))
print('Found %s labels.' % len(labels))
# 1136 texts and labels

# tokenizer.fit_on_texts looks at all the words in the corpus, assign index to unique words
# then tokenizer.texts_to_sequences converts the words in the corpus to the index which has been identified
tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)
tokenizer.fit_on_texts(clinical_texts)
sequences = tokenizer.texts_to_sequences(clinical_texts)


word_index = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index))
# 5060 unique tokens

data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
# data_test_sent = pad_sequences(test_sent, maxlen=MAX_SEQUENCE_LENGTH)
# shrink each of the 4000 text to 1000 numbers. Each number is the word index learned in the tokenization step

labels = to_categorical(np.asarray(labels))     #one-hot vectors for label categories
print('Shape of data tensor:', data.shape)      #1136,1000
print('Shape of label tensor:', labels.shape)   #1136,5

# split the data into a training set and a validation set
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]
num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])

x_train = data[:-num_validation_samples]
y_train = labels[:-num_validation_samples]
x_val = data[-num_validation_samples:]
y_val = labels[-num_validation_samples:]

print('Shape of training data:', x_train.shape)      #909,1000/5061
print('Shape of validation data:', x_val.shape)   #227,1000/5061



print('Preparing embedding matrix.')

# prepare embedding matrix
num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))     #5061,300
for word, i in word_index.items():
    # word is the key, i is the value
    if i >= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    # find the word in the pre-trained word vector dictionary
    # if the word is not found in the pre-trained word vectors, leave the embedding matrix as 0s
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector


# load pre-trained word embeddings into an Embedding layer
embedding_layer = Embedding(num_words,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=True)

print('Training model.')

# train a 1D convnet with global maxpooling
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
# this is x_train
embedded_sequences = embedding_layer(sequence_input)            #use embedding layer above for sequence_input
embedded_sequences = Dropout(0.2)(embedded_sequences)
x = Conv1D(128, 5, activation='relu')(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Dropout(0.3)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Dropout(0.3)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = GlobalMaxPooling1D()(x)
x = Dense(128, activation='relu')(x)
preds = Dense(len(idx2label), activation='softmax')(x)

model = Model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['acc'])

history = model.fit(x_train, y_train,
          batch_size=64,
          epochs=10,
          validation_data=(x_val, y_val))

model.summary()

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.ylabel('Accuracy score')
plt.xlabel('Number of times model is trained')
plt.title('Training and validation accuracy - With larger word embedding size')
plt.legend()

plt.figure()
```

Accuracy for validation set increased and loss decreased nicely.
<img src="/img/cnn_acc.png" width="500">
<img src="/img/cnn_loss.png" width="500">

### SVM
```python
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.decomposition import PCA
from scipy import sparse as sp
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.linear_model import SGDClassifier
from sklearn.multiclass import OneVsRestClassifier

text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1, 2), 
                                                  min_df=1, max_features=None, strip_accents='unicode',
                                                  analyzer="word", token_pattern=r'\w{1,}')), 
                         ('tfidf', TfidfTransformer(smooth_idf=1)),
                         ('clf-svm', SGDClassifier(loss='modified_huber', 
                                                   penalty='l2',
                                                   class_weight='balanced',
                                                   alpha=1e-3, tol=1e-3, max_iter=50))])
text_clf_svm = text_clf_svm.fit(X_train, y_train)
text_clf_svm = text_clf_svm.fit(X_train, y_train)
go = text_clf_svm.predict(X_test)
indices = np.where(go != y_test)[0]
print(indices)
print(y_test.iloc[indices])
```

